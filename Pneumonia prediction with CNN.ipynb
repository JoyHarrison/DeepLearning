{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7681529-7624-47b0-a467-c597f18c7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a993ae-2be1-4e4d-bd42-0695ac442f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176de2e-f1bc-4ae9-b9c2-c5e8f5104b5f",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7addbe-12e6-4ca4-8354-d1e12ccee904",
   "metadata": {},
   "source": [
    "Image augmentation to prevent Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf5c773-6cfa-405a-a42b-163894c6ce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create a tkinter window for the file dialog\n",
    "window = tk.Tk()\n",
    "window.withdraw()  # Hide the window\n",
    "window.attributes('-topmost', 1)  # Set window to be on top\n",
    "\n",
    "# Open a file dialog to let the user select the directory\n",
    "user_path = filedialog.askdirectory()\n",
    "\n",
    "# Close the tkinter window\n",
    "window.destroy()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# Use the user-selected path\n",
    "training_set = train_datagen.flow_from_directory(user_path,\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac58f47c-3635-4877-aa41-fb3132ab27c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create a tkinter window for the file dialog\n",
    "window = tk.Tk()\n",
    "window.withdraw()  # Hide the window\n",
    "window.attributes('-topmost', 1)  # Set window to be on top\n",
    "\n",
    "# Open a file dialog to let the user select the directory\n",
    "user_path = filedialog.askdirectory()\n",
    "\n",
    "# Close the tkinter window\n",
    "window.destroy()\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# Use the user-selected path\n",
    "test_set = train_datagen.flow_from_directory(user_path,\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f5918-5bd8-49fe-8302-35785ae00f2c",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e261de4-407a-4bf6-86d6-deb67f9a5a2e",
   "metadata": {},
   "source": [
    "Initializing CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f327e6b9-f505-4f9c-bb63-898561fc4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaba7db-758b-4fde-bcd4-f45ddfe7765e",
   "metadata": {},
   "source": [
    "Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab1932c-a8ac-41aa-959f-a4538e49b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28db6a-4a84-4467-94bc-d7f43f6cb55f",
   "metadata": {},
   "source": [
    "Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9525025-c9e8-4899-91df-669c59325696",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b5cd4-e7e0-4b3c-a2da-b8acdba0cf11",
   "metadata": {},
   "source": [
    "2nd Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5035113-7801-4286-a540-ca8bf31a0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b0a17-f006-48a2-9c5d-9e487f849a6d",
   "metadata": {},
   "source": [
    "Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da37e9d-9f0a-4d56-bd88-44b60d99f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80813b0b-d20a-4845-b59c-cabc05585847",
   "metadata": {},
   "source": [
    "Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7105de70-61f7-4793-8572-3b8a68885898",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b3f1c-fada-4ed0-9a2c-f06afdd4ea48",
   "metadata": {},
   "source": [
    "Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "329ea84f-e38c-42fe-a17e-69d96d49b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a8a8e-82ac-4453-8b9f-d4b22420f5f3",
   "metadata": {},
   "source": [
    "# Training CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d62cc-8afb-4cf1-a263-75add809dfdc",
   "metadata": {},
   "source": [
    "Compiling CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "365b611b-c7fc-4205-be4a-7a7c4c9342bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc00e86-f2a1-4703-ae81-81d51c86b4cd",
   "metadata": {},
   "source": [
    "Training CNN on training set and evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b08a027-8e71-4b87-b732-4e2d0119e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "163/163 [==============================] - 125s 757ms/step - loss: 0.3910 - accuracy: 0.8261 - val_loss: 0.4504 - val_accuracy: 0.7901\n",
      "Epoch 2/16\n",
      "163/163 [==============================] - 74s 457ms/step - loss: 0.2468 - accuracy: 0.8978 - val_loss: 0.4374 - val_accuracy: 0.7981\n",
      "Epoch 3/16\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.2137 - accuracy: 0.9132 - val_loss: 0.4473 - val_accuracy: 0.8061\n",
      "Epoch 4/16\n",
      "163/163 [==============================] - 68s 417ms/step - loss: 0.1950 - accuracy: 0.9201 - val_loss: 0.5289 - val_accuracy: 0.7756\n",
      "Epoch 5/16\n",
      "163/163 [==============================] - 85s 525ms/step - loss: 0.1608 - accuracy: 0.9377 - val_loss: 0.3166 - val_accuracy: 0.8622\n",
      "Epoch 6/16\n",
      "163/163 [==============================] - 80s 490ms/step - loss: 0.1595 - accuracy: 0.9385 - val_loss: 0.4380 - val_accuracy: 0.8125\n",
      "Epoch 7/16\n",
      "163/163 [==============================] - 76s 469ms/step - loss: 0.1560 - accuracy: 0.9390 - val_loss: 0.4094 - val_accuracy: 0.8413\n",
      "Epoch 8/16\n",
      "163/163 [==============================] - 79s 483ms/step - loss: 0.1540 - accuracy: 0.9410 - val_loss: 0.4336 - val_accuracy: 0.8429\n",
      "Epoch 9/16\n",
      "163/163 [==============================] - 75s 457ms/step - loss: 0.1422 - accuracy: 0.9423 - val_loss: 0.5570 - val_accuracy: 0.7981\n",
      "Epoch 10/16\n",
      "163/163 [==============================] - 76s 464ms/step - loss: 0.1503 - accuracy: 0.9429 - val_loss: 0.5123 - val_accuracy: 0.8061\n",
      "Epoch 11/16\n",
      "163/163 [==============================] - 74s 456ms/step - loss: 0.1307 - accuracy: 0.9477 - val_loss: 0.4864 - val_accuracy: 0.8205\n",
      "Epoch 12/16\n",
      "163/163 [==============================] - 75s 458ms/step - loss: 0.1271 - accuracy: 0.9505 - val_loss: 0.6928 - val_accuracy: 0.7772\n",
      "Epoch 13/16\n",
      "163/163 [==============================] - 78s 480ms/step - loss: 0.1398 - accuracy: 0.9482 - val_loss: 0.4390 - val_accuracy: 0.8478\n",
      "Epoch 14/16\n",
      "163/163 [==============================] - 87s 534ms/step - loss: 0.1288 - accuracy: 0.9507 - val_loss: 0.4141 - val_accuracy: 0.8478\n",
      "Epoch 15/16\n",
      "163/163 [==============================] - 76s 470ms/step - loss: 0.1330 - accuracy: 0.9457 - val_loss: 0.4582 - val_accuracy: 0.8253\n",
      "Epoch 16/16\n",
      "163/163 [==============================] - 79s 482ms/step - loss: 0.1258 - accuracy: 0.9517 - val_loss: 0.4460 - val_accuracy: 0.8526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d366060ed0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set,validation_data=test_set,epochs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97317457-71d2-4eb7-b22d-be6cfa2ce95e",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0282825-6f73-4444-994e-95e3cdf7b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NORMAL'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'D:\\JOY\\J AMITY\\Machine Learning\\pneumonia\\chest_xray\\val\\PNEUMONIA\\person1952_bacteria_4883.jpeg', target_size = (64, 64))\n",
    "\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'NORMAL'\n",
    "else:\n",
    "  prediction = 'PNEUMONIA'\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87dc0787-d5e4-4ce1-99e0-310b3a3cdc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PNEUMONIA'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'D:\\JOY\\J AMITY\\Machine Learning\\pneumonia\\chest_xray\\val\\NORMAL\\NORMAL2-IM-1436-0001.jpeg', target_size=(64, 64))\n",
    "\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'NORMAL'\n",
    "else:\n",
    "  prediction = 'PNEUMONIA'\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38932c65-96e9-4a91-9560-cbe09ef30e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
